id	texte	origine
0	Please post your personal projects, startups, product placements, collaboration needs, blogs etc.  Please mention the payment and pricing requirements for products and services.  Please do not post link shorteners, link aggregator websites , or auto-subscribe links.  \--  Any abuse of trust will lead to bans.  Encourage others who create new posts for questions to post here instead!  Thread will stay alive until next one so keep posting after the date in the title.  \--  Meta: This is an experiment. If the community doesnt like this, we will cancel it. This is to encourage those in the community to promote their work by not spamming the main threads.	reddit
1	**For Job Postings** please use this template  >Hiring: \[Location\], Salary:\[\], \[Remote | Relocation\], \[Full Time | Contract | Part Time\]    and \[Brief overview, what you're looking for\]  **For Those looking for jobs** please use this template  >Want to be Hired: \[Location\], Salary Expectation:\[\], \[Remote | Relocation\], \[Full Time | Contract | Part Time\]  Resume: \[Link to resume\] and \[Brief overview, what you're looking for\]  &#x200B;  Please remember that this community is geared towards those with experience.	reddit
2	"EDIT: this is really a question about the diffeomorphicity of continuous normalising flows and whether that is problematic (not about pictures of animals!)  Continuous normalising flows push a source distribution to a target distribution via a diffeomorphism (usually an automorphism of d-dimensional Euclidean space). I'm confused about sparsely sampled parts of the data distribution and whether the fact that the diffeomorphic mapping is assuming things about the data distribution (e.g. its connectivity) that aren't actually true (is it modelling the distribution too coarsely or is it learning the true distribution?).  E.g. let's say the data distribution has a lot of pictures of dogs and a lot of pictures of cats but no pictures of ""half dogs-half cats"" because they don't actually exist (note that there may be pictures of dogs that looks like cats but would sit in the cat picture part of the distribution -- dogcats do not exist in the real world). But the region in between the peaks of this bimodal distribution should be zero. But when we perform a diffeomorphic mapping from the source p (e.g., a Gaussian) part of the probability mass must be pushed to the intermediate part of the distribution. This is problematic because then we sample our q (by sampling p and pushing through the learned flow) we might end up with a picture of a halfdog-halfcat but that isn't physically possible.  What is going wrong here?  1. Is the assumption that our map is a diffeomorphism too restrictive, e.g., for topologically disconnected data distributions?  OR  2. Is the model faithfully learning what the intermediate regions of the data distribution look like? That seems magical because we haven't given it any data and in the example I've given it's impossible. Rather the diffeomorphic assumption gives us an intermediate part of the distribution that might be wrong because the true target distribution is topologically disconnected.  It seems of paramount importance that we know a priori about the topological structure of the data distribution -- no?  If you know any sources discussing this, that would be very helpful!  Many thanks!  [I'm interested in the intermediate region between the peaks](https://preview.redd.it/exchxfoolpwf1.png?width=870&format=png&auto=webp&s=9e2f0d950f589cdc81044707e6a3498fefec2c51)  [samples from the source distribution p \(e.g. Gaussian\) at t=0](https://preview.redd.it/rcafr6orlpwf1.png?width=568&format=png&auto=webp&s=136bfbd3136b90baddd5974823a0f6b00c35a43a)  [mid way through the flow 0\<t\<1](https://preview.redd.it/3iyuefvslpwf1.png?width=550&format=png&auto=webp&s=d51c4a7c2da085b4893bfd9927d3b238093a25ea)  [The target distibution q at t=1. I'm interested in the middle part of the distribution between the two peaks](https://preview.redd.it/5zn4v2ktlpwf1.png?width=557&format=png&auto=webp&s=5d8d5e0887cf0cb1b7ad2f6190b53e633bc7dd25)"	reddit
3	The web wasn't built for AI agents. It was built for humans with eyes, mice, and 25 years of muscle memory navigating dropdown menus.      Most AI companies are solving this with browser automation, playwright scripts, Selenium wrappers, headless Chrome instances that click, scroll, and scrape like a human would.  It's a workaround and it's temporary.  These systems are slow, fragile, and expensive. They burn compute mimicking human behavior that AI doesn't need. They break when websites update. They get blocked by bot detection. They're architectural debt pretending to be infrastructure etc.  The real solution is to build web access designed for how AI actually works instead of teaching AI to use human interfaces.   A few companies are taking this seriously. Exa or Linkup are rebuilding search from the ground up for semantic / vector-based retrieval Linkup provides structured, AI-native access to web data. Jina AI is building reader APIs for clean content extraction. Shopify in a way tried to address this by exposing its APIs for some partners (e.g., Perplexity)  The web needs an API layer, not better puppeteering.  As AI agents become the primary consumers of web content, infrastructure built on human-imitation patterns will collapse under its own complexity…	reddit
4	During the training of a neural network, a very common phenomenon is that of loss spikes, which can cause large gradient and destabilize training. Using a learning rate schedule with warmup, or clipping gradients can reduce the loss spikes or reduce their impact on training.  However, I realised that I don't really understand why there are loss spikes in the first place. Is it due to the input data distribution? To what extent can we reduce the amplitude of these spikes? Intuitively, if the model has already seen a representative part of the dataset, it shouldn't be too surprised by anything, hence the gradients shouldn't be that large.  Do you have any insight or references to better understand this phenomenon?	reddit
5	[https://www.scmp.com/tech/tech-trends/article/3328966/ai-powered-fraud-chinese-paper-mills-are-mass-producing-fake-academic-research](https://www.scmp.com/tech/tech-trends/article/3328966/ai-powered-fraud-chinese-paper-mills-are-mass-producing-fake-academic-research)  A new CCTV investigation found that paper mills in mainland China are using generative AI to mass-produce forged scientific papers, with some workers reportedly “writing” more than 30 academic articles per week using chatbots.      These operations advertise on e-commerce and social media platforms as “academic editing” services. Behind the scenes, they use AI to fabricate data, text, and figures, selling co-authorships and ghostwritten papers for a few hundred to several thousand dollars each.      One agency processed over 40,000 orders a year, with workers forging papers far beyond their expertise. A follow-up commentary in The Beijing News noted that “various AI tools now work together, some for thinking, others for searching, others for editing, expanding the scale and industrialization of paper mill fraud.”	reddit
6	**TL;DR**: I compress LLM context into **images** instead of text, and let a **vision-language model** (VLM) “decompress” it by reading the image. In my tests, this yields up to **\~2.8:1 token compression at 93.65% accuracy** on *Gemini 2.5-Flash-Lite (Exp 56)*, and **99.26% at 1.7:1** on *Qwen2.5-VL-72B-Instruct (Exp 34)*. Full code, experiments, and replication steps are open-source.  **Repo (please ⭐ if useful):** [https://github.com/MaxDevv/Un-LOCC](https://github.com/MaxDevv/Un-LOCC)  # What this is:  **Un-LOCC (Universal Lossy Optical Context Compression)**: a simple, general method to **encode long text context into compact images**, then **decode with a VLM**. Think of the VLM as an OCR-plus semantic decompressor.  * I render text into a fixed-size PNG (e.g., **324×324**, Atkinson Hyperlegible \~**13px**), pass that image to a VLM, and ask it to reproduce the original text. * **Accuracy** = normalized Levenshtein similarity (%). * **Compression ratio** = *text tokens ÷ image tokens*.  # Key results (linked to experiments in the repo):  * **Gemini 2.5-Flash-Lite**: **100% @ 1.3:1** *(Exp 46)* and **\~93.65% @ 2.8:1** *(Exp 56)*. * **Qwen2.5-VL-72B-Instruct**: **99.26% @ 1.7:1** *(Exp 34)*; **\~75.56% @ 2.3:1** *(Exp 41)*. * **Qwen3-VL-235B-a22b-Instruct**: **95.24% @ 2.2:1** *(Exp 50)*; **\~82.22% @ 2.8:1** *(Exp 90)*. * **Phi-4-Multimodal**: **94.44% @ 1.1:1** *(Exps 59, 85)*; **\~73.55% @ 2.3:1** *(Exp 61)*. * **UI-TARS-1.5-7B**: **95.24% @ 1.7:1** *(Exp 72)*; **\~79.71% @ 1.7:1** *(Exp 88)*. * **LLaMA-4-Scout**: **86.57% @ 1.3:1** *(Exp 53)*.  >Details, prompts, fonts, and measurement code are in the README. I cite each claim with **(Exp XX)** so you can verify quickly.  # Why this matters:  * **Cheaper context**: replace expensive text tokens with “image tokens” when a capable VLM sits in the loop. * **Architecturally simple**: no model modifications are needed, you can use rendering + a VLM you already have. * **Composable**: combine with retrieval, chunking, or multimodal workflows.  # What I need help with:  * **A better algorithm:** The O-NIH algorithm is okay for checking if models can see the text, however I'm not sure how to easily determine the model's full comprehension of the text. * **Model coverage**: more open VLMs; local runs welcome. * **Edge cases**: math, code blocks, long tables, multilingual. * **Repro/PRs**: if you get better ratios or accuracy, please open an issue/PR.  **Repo again (and yes, stars genuinely help discoverability):** [https://github.com/MaxDevv/Un-LOCC](https://github.com/MaxDevv/Un-LOCC)	reddit
7	Hi everyone.  For the past couple of weeks I have been playing around with PI0.5 and training it on behavior 1k tasks. I performed a full fine-tuning training run of PI0.5 for 30000 steps with batch size of 32 and it took 30 hours.  In order for me to train over 1 epoch of the entire behavior 1k dataset with batch size of 32 I need to perform 3.7 million training steps. This will take around 3700 hours or 154 days which would amount to $8843 ($2.39 for 1 H100).  So I decide to optimize the training script to improve the training time and so far I have been able to achieve 1.4x speedup. With some more optimizations 2x speedup is easily achievable. I have added a small video showcasing the improvement on droid dataset.  [https://yourimageshare.com/ib/KUraidK6Ap](https://yourimageshare.com/ib/KUraidK6Ap)  After a few more optimizations and streamlining the code I am planning to open-source it.	reddit
8	Good talk by Sergey Levine about the current state-of-the-art in robotic foundation models: https://www.youtube.com/watch?v=yp5fI6gufBs  TL;DR They use a pretrained VLM, stapled to a diffusion or flow model trained on robotics actions. Reinforcement learning inside the latent space of a diffusion model is surprisingly efficient compared to traditional RL (as few as 50 rollouts with sparse rewards).   This works well, but the primary bottleneck is a lack of large action datasets.  Much more research and data collection will be necessary to build practical robots.	reddit
9	Hi everyone. I'd like to share something I've been working on: Attention-Driven Transformers for time series forecasting  The approach focuses on maximizing attention's representational capacity by using a single top-layer attention block O(n²) to drive multiple lightweight projection blocks O(n), rather than repeating full attention across all blocks. It uses PatchTST's patching algorithm to segment time series into overlapping windows.  The core insight is that attention works best as a global organizational mechanism, not necessarily something you need implemented in every block. The model also uses multiplicative positional encoding rather than additive, which scales features by learned positional weights.  The architecture consistently improves performance over PatchTST (a SOTA baseline) across standard benchmarks while being 1.3-1.5x faster, with improvements ranging from 1-20% depending on the dataset.  Code and full details can be found here: [https://github.com/pfekin/attention-driven-transformers](https://github.com/pfekin/attention-driven-transformers)	reddit
10	We present rBridge, a method that enables small proxy models (≤1B parameters) to effectively predict large-model reasoning performance, addressing the emergence problem in reasoning capabilities.  **Paper:** [https://www.arxiv.org/abs/2509.21013](https://www.arxiv.org/abs/2509.21013)  **Abstract/TL;DR:** Given the prohibitive cost of pre-training large language models, leveraging smaller proxy models to optimize datasets before scaling up is essential. However, reasoning capabilities exhibit emergent behavior only at larger scales (typically >7B parameters), making traditional proxy approaches ineffective. rBridge solves this by aligning evaluation with both (1) the pre-training objective and (2) the target task through weighted negative log-likelihood using frontier model reasoning traces.  **Key Contributions:**  1. **Theoretical insight:** We identify that proxy evaluation schemes must align with both pre-training objectives and target tasks for effective reasoning prediction 2. **Novel method:** rBridge weights NLL by task-alignment using frontier model confidence scores, handling tokenizer mismatches at letter-level 3. **Empirical validation:**    * 100.2× compute reduction for dataset ranking (80.8% decision accuracy across 25 datasets)    * Strong proxy-target correlations: R² = 0.826-0.874 across 6 benchmarks (GSM8K, MATH500, ARC-C, MMLU Pro, CQA, HumanEval)    * Zero-shot transfer of fitted functions across pre-training datasets  **Experimental Setup:**  * Proxy scales: 100M to 1B * Target scales: 7B to 32B * Training corpus: 250B to 3.75T tokens * Evaluation: 5-fold cross-validation  **Practical Impact:** This enables compute-constrained researchers to explore pre-training design choices at dramatically reduced costs. A single 7B training run can exceed $50K; our method reduces exploration costs by 100×+ while maintaining predictive accuracy.  Code will be released soon.	reddit
11	[https://arxiv.org/abs/2402.09267](https://arxiv.org/abs/2402.09267)  Very interesting paper I found about how to make LLMS keep themselves in check when it comes to factuality and how to mitigate and reduce hallucinations without the need of human intervention.  I think this framework could contribute and give LLMs huge benefits, especially in fields where high factuality confidence and low (or ideally none) hallucinations are needed.  Summary: In this work, we explore Self-Alignment for Factuality, where we leverage the self-evaluation capability of an LLM to provide training signals that steer the model towards factuality. 	reddit
12	Hi,  Sorry for the non-learning question, but most of the community is here.  There's ' upstream request timeout' on OpenReview. Has been for a while.  Are you experiencing that too? Do you have an idea on the ETA on the uptime?  Appreciated!	reddit
13	"Quick context: I'm training a playable DOOM world model where you can prompt like ""spawn cyberdemon left"" or ""harder"" to change game events in real time. I wanted to take DeepMind's playable Doom world model in [Diffusion Models are Real-Time Game Engiens](https://arxiv.org/abs/2408.14837), and add text conditioning to make game events promptable.  **To train this I need \~100 hours of action-labeled DOOM gameplay data.**  I could have scraped DOOM data from YouTube, or paid contractors, but thought it would be fun to train a curious RL agent that explores the map. I thought this would be a solved problem, since I saw RL papers from 2018 about ""curiosity-driven"" learning.  I couldn't have been more wrong! Training agents to be ""curious"" is far from a solved problem. Here's what I tried and what happened so far:  **1. Implemented the original** [**curiosity-driven exploration**](https://arxiv.org/abs/1705.05363) **paper(Pathak et al., 2018) → hit the Noisy TV Problem**  The Noisy TV Problem is where the agent gets stuck staring at a random process in the game. This is a known problem with defining the curiosity bonus as prediction error, since noise is not learnable. The specific ""Noisy TV"" the agent converges to is getting transfixed by the pistol's muzzle smoke against a high-contrast white background.  **2. Implemented** [Learning Progress Monitoring](https://arxiv.org/pdf/2509.25438v1) **(2025) → agent converged to taking no action.**  The paper defined curiosity bonus as learning progress: difference between past prediction error of next state and current prediction error of next state. Sounds good on paper, but in practice you have to get a lot right to guarantee past prediction error > current prediction error for learnable (non-random) states. I couldn't figure this out, and past and present prediction error became roughly equal during training, causing agent to take no action due to lack of reward.  **3. Implemented OpenAI** [Random Network Distillation](https://arxiv.org/abs/1810.12894) **→ agent learns  but not because of curiosity**  The agent learned, but only because of extrinsic rewards (kills, room discovery, etc), not curiosity bonus rewards. After many iterations, curiosity bonus rewards shrank to zero as well, similar to LPM. The agent acts greedily to kill enemies and discover rooms, with little to no variety in its actions.  More details here in my repo, where all three implementations work out-of-box: [https://github.com/pythonlearner1025/BoredDoomGuy](https://github.com/pythonlearner1025/BoredDoomGuy)  At this point, I reminded myself training a curious RL agent is a side quest, and I have to get back on the main quest. But if you've trained an agent to complete Doom E1M1 purely from curiosity, I'm curious to hear how you did it!  For now, I'm falling back to collecting training data from human players. You can help by playing doom in your browser at [playdoom.win](https://www.playdoom.win) your fun is my training data: your game viewport and actions will be logged!"	reddit
14	I’ve been digging into how researchers build datasets for code-focused AI work — things like program synthesis, code reasoning, SWE-bench-style evals, DPO/RLHF. It seems many still rely on manual curation or synthetic generation pipelines that lack strong quality control.  **I’m part of a small initiative supporting researchers who need custom, high-quality datasets for code-related experiments — at no cost. Seriously, it's free.**  If you’re working on something in this space and could use help with data collection, annotation, or evaluation design, I’d be happy to share more details via DM.  Drop a comment with your research focus or current project area if you’d like to learn more — I’d love to connect.	reddit
15	What bias variance tradeoff teaches us:   We must carefully limit the power of our models to match the complexity of our data to avoid overfitting.   When we make Neural Networks larger it works better which contradicts our bias variance tradeoff which is actually incomplete.  Keeping the dataset fixed and no early stopping as we increasing the NN size:  When we make a NN larger at the start the performance increases rapidly, than if we continue to make it larger at some point the performance starts to get worse(starts to overfit) and it gets worst exactly at the interpolation point(0 training error/ model has 1:1 correspondence with the dataset). And after this point the test error again start to decrease creating a second descent.  To explain its cause:   When model capacity is low you underfit (high bias). As capacity rises toward the **interpolation threshold** (capacity ≈ training data degrees of freedom) the model can exactly fit the training data, so tiny changes in training data can lead to large fluctuations in the learned parameters and predictions, causing the validation or test error to spike sharply due to high variance.   Before the interpolation point when there is lot more dataset as compared to model complexity, the model learns to ignore the noise and only capture the most relevant patterns as it doesn't have enough parameters.   **Overparameterized region:**  with many more parameters than data, there are infinitely many zero-training-error solutions; optimization (and explicit regularizes like weight decay or implicit biases of SGD) tends to select low-complexity/low-norm solutions, so test error can drop again ->**double descent**.	reddit
16	Hey,  When I prepare my NeurIPS submission camera-ready version, I found that the instruction email asks to put the checklist before the appendices.  However, in this call for paper page (https://neurips.cc/Conferences/2025/CallForPapers), the LaTex style file actucally put the checklist after the appendices.   Personally speaking, putting the checklist before appendices is not aesthetic and elegant. I also check around 30 camera ready NeurIPS papers that got uploaded to arXiv, and only one put the checklist before appendices (although most of the accepted paper don't even include checklist on arXiv version.)  I'm just want to check if anyone have any idea how strict these instruction will be? If I put the checklist after appendices, will I get 'reject'? (I guess the chance is very small but just want to double-check). 	reddit
17	When working on various recommender systems, it always was weird to me that creating dashboards or doing feature engineering is hard with integer-valued features that are heavily tailed and have large support, such as # of monthly visits on a website, or # monthly purchases of a product.     So I decided to do a one small step towards tackling the problem. I hope you find it useful:   https://arxiv.org/abs/2510.15132	reddit
18	"ICLR 2026 author guide says max 9 pages of main text in submissions, while FAQ says 10 pages. And Google shows several such contradictions in time and space...  Vanilla definition of ""main text"" is all content between title and references, except for exempt sections, i.e. ""Ethics"" and ""Reproducibility"" sections per author guide.  Random sampling suggests \~5% of the \~20,000 submissions under review have main text on page 10. Would you  1. Allow all submissions with main text on page 10 2. Disallow all submissions with main text on page 10 3. Subjectively allow/disallow submissions with main text on page 10  PS: will adhere to the top-ranked answer in my reviews"	reddit
19	Dear fellow ML people,  LLMs need trillions of tokens to be trained, which makes optimization and speed key of current ML pipeline. When I wrote a [GPT2 implementation from scratch](https://github.com/Bornlex/GPT2), I iteratively improved it by adding a few features such as Multi-head self attention, grouped query self attention, kv cache...  Then I asked myself : can I make training faster ?  I wrote this blog article [Make GPU go brrr](https://bornlex.github.io/posts/triton1/) a few days ago and would be very happy to know :  1. **How useful is it to you ?** I try to write articles to compile multiple sources online so that readers get a 0 to 1 resource. It helps me clear my mind, serialize my knowledge somewhere, and hopefully land a big AI company job someday ! 2. **How can I improve it ?** Feel free to share feedback about the quality of the writing, if something is not clear, if the drawings are too cryptic... 3. **What topic should I focus on next ?** This one is purely for me to improve even more thanks to you guys.  During this journey of writing articles, I find myself digging deeper and deeper into technical stuff, which is very exciting. This Triton part of ML is lovely and allows me to make converge 2 sides of computer science that I love : AI and low level programming. I will iterate on this with an implementation of FlashAttention.  Have a great week.  Cheers.	reddit
20	Hey everyone,  I got tired of seeing interesting plots in papers and then spending 30+ minutes hunting through GitHub repos or trying to reverse-engineer the visualization code, so I built a tool to fix that.  **What it does:**  * Browse a searchable gallery of plots from ML papers (loss curves, attention maps, ablation studies, etc.) * Click any plot to get the exact Python code that generated it * Copy-paste the code and run it immediately - all dependencies listed * Filter by model architecture, or visualization type and find source papers by visualization  The code snippets are self-contained and include sample data generation where needed, so you can actually run them and adapt them to your own use case using LLM agents as well.  [Be an early user :)](https://ml-builder.vercel.app/)  Right now it has \~80 plots from popular papers (attention mechanisms, transformer visualizations, RL training curves, etc.) but I'm adding more weekly. If there's a specific paper visualization you always wanted to replicate, drop it in the comments and I'll prioritize it.  Happy to answer questions about implementation or take suggestions for improvements!	reddit
21	"I am interested in creating something---much simpler than Deep Research---that will use web search to fetch statistics such as ""How many DUIs occur each year in the United States?"" I am looking for a framework that allows me to use different LLMs to power it (e.g., can sub in openai, llama, etc). Any advice on what framework/library to use?"	reddit
22	Any steps that have worked for you in the past will work. My generator loss is around 2-3 range (with identity and cyclic components), while discriminator loss has flat lined at 0.005-0.02. Sample outputs look extremely different from what is required. After a certain epoch, I implemented 2x Gen step for each disc, higher gen loss, lowered cyclic and identity components, but 2-3 epoch later, even if the gen loss is less, there isnt any change in disc loss  [](https://www.reddit.com/submit/?post_id=t3_1obf0ky)	reddit
23	I was going through the triton tutorial for vector addition [here](https://triton-lang.org/main/getting-started/tutorials/01-vector-add.html#sphx-glr-getting-started-tutorials-01-vector-add-py). When I added `torch.cuda.synchronize()` statement before `return output` in the add function, the benchmarks showed that the difference between the triton and torch implementations blew up. I was under the impression that `synchronize()` would just wait for all the threads to finish running before returning the output, but clearly something is going wrong. Could anyone explain what is going on?	reddit
24	"A couple quotes from Gemini and Claude  ""While still in high demand, some of the model-specific work is becoming more democratized or abstracted by automated tools and APIs.""  """"""  The ML engineering that remains valuable:  * Research-level work at frontier labs (extremely competitive, requires PhD + exceptional talent) * Highly specialized domains (medical imaging, robotics, etc.) where you need domain expertise + ML * Infrastructure/systems work (distributed training, optimization, serving at scale) * Novel applications where APIs don't exist yet  The ML engineering that's being commoditized:  * Standard computer vision tasks * Basic NLP fine-tuning * Hyperparameter optimization * Model selection for common tasks * Data preprocessing pipelines  """"""  Is the job landscape bifurcating toward: (1) research + frontier labs, (2) applying off-the-shelf models to business verticals  My background:  I left a computer vision role several years ago because I felt like it was plateauing, where all I was doing was dataset gathering and fine-tuning on new applications. It wasn't at a particularly stellar company.  I went to a more general data science & engineering type role, more forecasting and churn focused.  I'm debating whether to try to upskill and foray into AI engineering, building RAG systems.  What are y'all's thoughts? How does one go about doing that jump? Maybe the MLE roles are still stable and available, and I just need to improve."	reddit
25	I'm a reviewer (PC) and don’t have a submission myself, but honestly, this is the weirdest reviewing process I’ve ever experienced.      1. Phase 2 papers are worse than Phase 1.    In Phase 1, I reviewed four papers and gave scores of 3, 4, 5, and 5. I was even open to raising the scores after the discussion, but all of them ended up being rejected. Now, in Phase 2, I have papers rated 3 and 4, but they’re noticeably weaker than the ones from Phase 1.  2. It feels like one reviewer is personally connected to a paper.   I gave a score of 3 because the paper lacked technical details, justifications, and clear explanations for inconsistencies in conventions. My review was quite detailed—thousands of characters long—and I even wrote another long response after the rebuttal. Meanwhile, another reviewer gave an initial rating of 7 (confidence 5) with a very short review, and later tried to defend the paper and raise the score to 8. That reviewer even wrote, *“The authors have clearly addressed most of the reviewers' concerns. Some experimental questions were not addressed due to regulatory requirements.”* But I never raised any experimental questions, and none of my concerns were actually resolved.  \+ actually this paper's performance looks very good, but 'paper' is just not about performance.     Should I report this somewhere? If this paper is accepted, I'll be very disappointed and will never submit or review a paper from AAAI. There are tons of better paper.	reddit
26	"I've figured out the error that was published several years ago. The paper provides a convergence theorem of fundamental algorithm. The key theorem relies on the specific Lemma, however, I figured out that invoking this lemma is a ""bit"" misleading. They should add a bit stronger assumption (which, I do not think it is that strong) to invoke such lemma.   However, due to this issue, the key theorem does collapse.  What should I do?"	reddit
27	Hi everyone,  I’ve noticed that most discussions lately revolve around LLMs and NLP, but I’m curious about what other areas in AI/ML are currently getting attention in research.  What topics or fields do you think are becoming exciting right now?	reddit
28	Hey everyone,  I’m currently working on my Master’s thesis on *cloud removal from optical satellite imagery*, and I’m exploring the use of **Rectified Flow (RF)** models for this task. Most existing approaches use CNNs, diffusion models (like DiffCR), or multi-temporal transformers, but rectified flows seem promising because they can produce high-quality results in fewer steps than diffusion while maintaining stability and smooth transport.  My idea is to train a **conditional rectified flow** that maps cloudy → cloud-free images, conditioned on auxiliary inputs like cloud masks, temporal neighbors, or even SAR data for thick clouds. I’m considering both **pixel-space** and **latent-space** RF formulations (using a pretrained VAE or autoencoder).  I’m curious about:  * Whether anyone has seen similar work applying rectified flows to image restoration or remote sensing tasks. * Any tips on stabilizing conditional training for RFs or improving sample efficiency. * Open datasets/papers you’d recommend for realistic multi-temporal or SAR-optical cloud removal benchmarks(some i know of are sentinel dataset,  landsat etc)  Would love to discuss architectures, loss formulations, or evaluation strategies (PSNR/SSIM/SAM/FID) if anyone’s experimenting in this space.  Thanks in advance!	reddit
29	"Years back, after finishing my CS degree, I got into algorithmic trading as a personal project. It felt like the perfect arena to push my skills in coding, data science, and, most importantly, data engineering. After a long road of development, I recently deployed my first fully automated, ML-driven system.  The trading results aren't the point of this post. I'm here to talk about the steps I've taken to solve the fundamental problem of getting a machine learning model to perform in a live environment exactly as it did during historical testing.  A live production environment is hostile to determinism. Unlike a sterile backtest where all data is known, a live system deals with a relentless, ordered stream of events. This introduces two critical failure modes:  * **Lookahead Bias:** The risk of accidentally using information from the future to make a decision in the past. A live system must be architected to be a strict ""tape reader,"" ensuring it only ever acts on information that has already occurred. * **State Drift:** A more insidious problem where the system's internal ""memory""—its representation of the world, built from the stream of incoming data—slowly but surely drifts away from the ground truth of the historical environment. The live model ends up seeing a distorted reality compared to the one it was trained on, rendering its predictions meaningless.  It's important to note that training a model on features containing lookahead bias will often *cause* state drift, but not all state drift is caused by lookahead bias. My entire development process was engineered to prevent both.  My first principle was to enforce a strict, row-by-row processing model for all historical data. There are countless ways lookahead bias can creep into a feature engineering pipeline, but the most tempting source I found was from trying to optimize for performance. Using vectorized pandas operations or multi-threading is standard practice, but for a stateful, sequential problem, it's a minefield. While I'm sure there are pandas wizards who can vectorize my preprocessing without causing leaks, I'm not one of them. I chose to make a deliberate trade-off: I sacrificed raw performance for provable correctness.  My solution is a ""golden master"" script that uses the *exact same stateful classes* the live bot will use. It feeds the entire historical dataset through these classes one row at a time, simulating a live ""tape reader."" At the end of its run, it saves the final state of every component into a single file. While this is much slower than a vectorized approach, it's the cornerstone of the system's determinism.  The live bot's startup process is now brutally simple: it loads the state file from the golden master. It doesn't build its own state; it *restores* it. It only has to process the short data gap between the end of the golden master's run and the current moment. This makes the live system easier to debug and guarantees a perfect, deterministic handover from the historical environment.  Finally, I have the validator. This tool also starts from the same ""golden master"" state and re-processes the exact same raw data the live bot saw during its run. The goal is a Pearson correlation of 1.0 between the live bot's predictions and the validator's predictions. Anything less than a perfect correlation indicates a logical divergence that must be found and fixed.  This project has been an incredible learning experience, but the biggest lesson was in humility. The most complex challenges weren't in model architecture but in the meticulous data engineering required to create a provably consistent bridge between the historical and the live environments.  While my actual trading models are private, I have a lower-frequency version of the system that posts market updates and predictions. After running live for over three weeks, it maintained a >0.9999 correlation with its validator - shown in the attached picture. It's currently offline for some upgrades but will be back online in a few days. You can see it here:  [https://x.com/ZtenlEssej](https://x.com/ZtenlEssej)  Thanks for reading. I have high hopes for my trading system, but it will take time. For now my skills are very much for hire. Feel free to reach out if you think I could be a fit for your project!"	reddit
30	"Hi everyone,  I'm starting a project to train a reinforcement learning agent that can operate a desktop computer, with the eventual goal of performing multi-step tasks. I have a good grasp of RL theory but I'm hitting a wall trying to find a suitable environment to actually train and benchmark my agent.  I'm looking for something that mimics a real desktop interaction, but in a controlled setting. Here’s a breakdown of what I need:  **1. Observation Space:**   The observation should be a representation of the current screen state. I'm open to different approaches:  * **Pixel-based:** A screenshot of the desktop/virtual machine. This is the most general form. * **DOM/HTML-based:** If the environment is web-focused, the HTML source code of the current page would be a fantastic, more structured alternative to pixels. * **Accessibility Tree:** Something like the UI hierarchy from Windows' UI Automation or Apple's Accessibility APIs would also be great.  **2. Action Space:**   The agent needs to perform low-level actions, similar to a human user:  * **Mouse:** Move to (x, y) coordinates, left/right/middle click, click-and-drag, scroll. * **Keyboard:** Send keystrokes (both text and special keys like `ENTER`, `TAB`).  **3. The Crucial Part: A Benchmark Suite**   This is where I'm really struggling. I don't just need an empty environment; I need a **curated set of tasks** to define success and measure progress. Ideally, this would be a suite of tasks with a clear reward signal.  **Example tasks I have in mind:**  * **Web Tasks:**    * ""Log into Gmail.""    * ""Search for a product on Amazon and add it to your cart.""    * ""Find the contact email on a company's 'About Us' page."" * **Desktop Application Tasks:**    * ""Open a text editor, write a sentence, and save the file to the desktop.""    * ""Create a new calendar event for tomorrow at 3 PM.""  I've looked at environments like `miniwob++`, which is a great start and almost exactly what I need for web tasks, but I'm wondering if there's anything more robust, more modern, or that extends beyond the browser to the full desktop OS.  **My Questions:**  1. Does a ready-to-use environment like this already exist? (e.g., a ""DesktopGym"" or ""WebShoppingSuite-v0""?) 2. If not, what would be the best way to build one? Is it better to create a virtual machine and use image-based observations, or is there a framework for hooking into a browser/OS to get a more structured observation space? 3. Are there any known research projects or benchmarks that have tackled this specific problem of a general desktop agent?  Any pointers to papers, GitHub repos, or existing projects would be immensely appreciated. Thanks in advance"	reddit
31	"We implemented Stanford's recent ""Agentic Context Engineering"" paper (https://arxiv.org/abs/2510.04618) and open-sourced it.   Instead of fine-tuning, agents curate their own context by learning from execution feedback. Three-agent system (Generator, Reflector, Curator) builds a ""playbook"" of strategies autonomously.   GitHub: https://github.com/kayba-ai/agentic-context-engine   Interested in feedback from the community on the approach and implementation!"	reddit
32	i have the option to take a numerical analysis class next semester, and I wanted to ask, what are some cool applications of machine learning and deep learning with numerical analysis? And what jobs combine ML and numerical analysis techniques?	reddit
33	I built and trained this very simple MoE \[ [Beens-MiniMax](https://github.com/Abinesh-Mathivanan/beens-minimax) \] from scratch in a span of 5 days. You could read more in the [report](https://github.com/Abinesh-Mathivanan/beens-minimax/blob/main/Beens_MiniMax__How_not_to_Build_an_LLM.pdf) here.	reddit
34	"We’re releasing **Kanops Open Access · Imagery (Retail Scenes v0)**: \~10k+ retail photos (UK/US supermarkets; fixtures, shippers, pumpkins/seasonal, signage).   Faces are blurred;   EXIF/IPTC carries provenance.   Dataset is **gated for evaluation use** (no redistribution/model-weight redistribution).  * HF dataset: [https://huggingface.co/datasets/dresserman/kanops-open-access-imagery](https://huggingface.co/datasets/dresserman/kanops-open-access-imagery) * Structure: train/{2014, FullStores, Halloween2024}/Retailer/Subcategory/\*.jpeg * Files: MANIFEST.csv, metadata.csv, checksums.sha256, LICENSE, [README.md](http://README.md)  **Intended tasks:** scene understanding for retail (bay detection, planogram reasoning, signage classification, seasonal, OCR-on-shelves plus other use cases around retail shelf fill and other use cases......   **Quick load (imagefolder):**  **# pip install datasets**  **from datasets import load\_dataset**  **ds = load\_dataset(""imagefolder"", data\_dir=""hf://datasets/dresserman/kanops-open-access-imagery/train"")**  **print(len(ds\[""train""\]))**  **Roadmap (v1):** add weak labels (orientation, aspect, season) and CVAT tags.  **Contact:** [happytohelp@groceryinsight.com](mailto:happytohelp@groceryinsight.com)  Happy to answer questions + consider task suggestions."	reddit
35	Do we know when the presentation schedule for NeurIPS 2025 (San Diego) is announced? I will have some travel conflicts with another conference, so trying to get some details.	reddit
36	Has anyone used [torchax](https://github.com/google/torchax) to run pytorch modules in jax and vice versa? It looks like a good solution to use the jit compiler for pytorch function. [https://youtu.be/Ofn-PLF1ej0?t=1007](https://youtu.be/Ofn-PLF1ej0?t=1007)	reddit
37	New episode of Learning from Machine Learning with Dan Bricklin, co-creator of VisiCalc, the first electronic spreadsheet that launched the personal computer revolution. His insight on breakthrough innovation: innovations must be 100 times better, not incrementally better.  His framework is simple. When evaluating if something truly matters, ask:  - What is this genuinely better at? - What does it enable that wasn't possible before? - What trade-offs will people accept? - Does it pay for itself immediately?  These same questions made spreadsheets inevitable and apply directly to AI today. But the part that really hit: Bricklin talked about the impact you never anticipate. A mother whose daughter with cerebral palsy could finally do her own homework. A couple who met learning spreadsheets. These quiet, unexpected ways the work changed lives matter more than any product launch or exit.  When we build something, we chase metrics and milestones. We rarely imagine the specific moments where what we made becomes essential to someone's life in ways we never predicted.	reddit
38	Hi everyone,  I'm hoping to get a sense of what ML/AI fields are the focus of active research and development in the private sector today.  I currently work as a Data Scientist (finished my Ph.D. two years ago) and am looking to transition into a more research-focused role. To guide my efforts, I'm trying to understand which fields are in demand and what knowledge would make me a stronger candidate for these positions.  My background is strong in classical ML and statistics, so not much of NLP or CV, even though I did learn the basics of both at some point. While I enjoy these classical areas, my impression is that they might not be in the spotlight for *new* research roles at the moment. I would be very happy to be proven wrong!  If you work in an industry research or applied science role, I'd love to hear your perspective. What areas are you seeing the investment and hiring in? Are there any surprising or niche fields that still have demand?  Thanks in advance for your insights!	reddit
39	"**TL;DR:** Tool-call accuracy in LLMs can be significantly improved by using natural language instead of JSON-defined schemas (\~+18 percentage points across 6,400 trials and 10 models), while simultaneously reducing variance by 70% and token overhead by 31%. We introduce Natural Language Tools (NLT), a simple framework that decouples tool selection from response generation and eliminates programmatic format constraints and extends tool calling to models even without tool-call support.  **Resources:** [Paper](https://arxiv.org/abs/2510.14453)  **Authors:** Reid T. Johnson, Michelle D. Pain, Jordan D. West  # The Problem  Current LLMs use structured JSON/XML for tool calling, requiring outputs like:      {       ""tool_calls"": [{         ""name"": ""check_talk_to_a_human"",         ""description"": ""Used when the user requests...""       }]     }  This structured approach creates three  bottlenecks:  1. **Task interference**: Models must simultaneously handle multiple tasks, such as understanding queries, select tools, maintaining format constraints, and  generating responses. 2. **Format burden**: Research demonstrates that the more structured a model's output, the more its performance tends to degrade ([a great paper by Tam on the subject](https://arxiv.org/abs/2408.02442)). 3. **Context bloat**: Structured schemas increase token usage, since you define not only the tool name and description, but surrounding JSON or XML syntax.  Even when tool selection is separated from response generation, probability mass is diverted toward maintaining correct formatting rather than selecting the right tools.  # Method: Natural Language Tools (NLT)  We introduce a simple three-stage framework that replaces JSON with natural language:  [Example NLT architecture with Selector \> Parser \> Output](https://preview.redd.it/o80vloo1ylvf1.jpg?width=2259&format=pjpg&auto=webp&s=3c75d8e6986fd499c61ebb364acb4c69abbaf157)  **Stage 1 - Tool Selection:** Model thinks through if any tools are relevant, then lists each tool with a YES/NO determination:      Thinking: (brief reasoning)     Example Tool 1 - YES/NO     Example Tool 2 - YES/NO     Example Tool 3 - YES/NO     Assessment finished.  **Stage 2 - Tool Execution:** Parser reads YES/NO decisions and executes relevant tools  **Stage 3 - Response:** Output module receives tool results and generates final response  **Evaluation:** 6,400 trials across two domains (Mental Health & Customer Service), 16 inputs per domain, 5 repetitions per input. Both original and perturbed inputs were tested to control for prompt engineering effects.  # Results  We find that NLT significantly improves tool-call performance, boosting accuracy by more than 18 percentage points (69.1% to 87.5%). Variance overall fell dramatically, falling more than 70% from .0411 to .0121 when switching from structured tool calling to NLT.  DeepSeek-V3 was a standout example, jumping from 78.4% to 94.7% accuracy while its variance dropped from 0.023 to 0.0016, going from among the least stable to the most consistent performer.  While we couldn't compare relative gain, NLT extends tool calling to models without native tool calling support (DeepSeek-R1: 94.1% accuracy).  # Basic NLT Template  **Basic NLT Prompt Template:**      You are an assistant to [Agent Name], [context].          Your mission is to identify if any of the following topics have      been brought up or are relevant:          - Tool 1 (description of when to use it)     - Tool 2 (description of when to use it)     ...          Your output should begin by thinking whether any of these are      relevant, then include the name of every tool followed by YES or NO.      End with ""Assessment finished.""          Format:     Thinking: (reasoning)     Tool 1 - YES/NO     Tool 2 - YES/NO     ...     Assessment finished.  Full prompts and implementation details in [Appendix A](https://arxiv.org/abs/2510.14453). Works immediately with any LLM with no API changes or fine-tuning needed.  # Limitations  **Latency considerations:** NLT requires minimum two model calls per response (selector + output), whereas structured approaches can respond immediately when no tool is needed.  **Evaluation scope:**  We examined single-turn, parameterless tool selection. While less complex than existing multi-turn benchmarks, it proved sufficiently rigorous -- no model achieved 100% accuracy in either condition.  A full discussion on limitations and areas for further research can be found in section 5.9 of the paper!  # Discussion & Implications  We propose five mechanisms for these improvements:  1. **Reduced format burden**: Requiring structured outputs (e.g. JSON) may divert the model's probability mass toward syntax control rather than task accuracy 2. **Reduced task interference**: By separating the tool selection into its own distinct stage, task interference can be  sidestepped. 3. **Training alignment**: The majority of model training is on outputting human-readable text, and NLT better aligns with this training paradigm. This is further supported by our results, as open-weight models see more pronounced gains. This makes intuitive sense, as open-weight models typically have fewer resources to invest in structured tool-call training. 4. **Explicit full-catalog consideration**: Requiring the model to explicitly include each tool name in its output avoids positional bias, allowing the model to ""recollect"" each tool right before it makes a determination. 5. **Reduced context length**: Even minor increases in tokens can degrade performance, and NLT used 47.4% fewer input tokens on average than its structured tool call counterpart (largely due to removing JSON boilerplate).  For agentic systems, the NLT approach could significantly boost tool selection and accuracy, particularly for open-source models. This may be especially relevant for systems-critical tool call capabilities (i.e. safety).  For model trainers, training efforts currently devoted to SFT and RLHF for structured tool calls may be better directed toward natural-language approaches. This is less clear, as there may be cross-training effects.  One of the authors here, happy to answer any questions about experimental design, implementation, or discuss implications! What do you think?"	reddit
40	Hi guys,  I just released the source code of my most recent project: a DQN network controlling the radiator power of a house to maintain a perfect temperature when occupants are home while saving energy.  I created a custom gymnasium environment for this project that relies on thermal transfer equation, so that it recreates exactly the behavior of a real house.  The action space is discrete number between 0 and max\_power.  The state space given is :  \- Temperature in the inside,  \- Temperature of the outside,  \- Radiator state,  \- Occupant presence,  \- Time of day.  I am really open to suggestion and feedback, don't hesitate to contribute to this project !  [https://github.com/mp-mech-ai/radiator-rl](https://github.com/mp-mech-ai/radiator-rl)  EDIT: I am aware that for this linear behavior a statistical model would be sufficient, however I see this project as a template for more general physical behavior that could include high non-linearity or randomness.	reddit
41	Hi all  I have a dilemma I really need help with. My old macbook pro died and I need a new one ASAP, but could probably hold off for a few weeks/months for the macbook pro 5 pro/max. I reserved the Nvidia DGX months ago, and I have the opportunity to buy it, but the last date I can buy it is tomorrow. I can also buy GCP credits.  Next year my research projects will mainly be inference of open source and closed source LLMs, with a few projects where I develop some multimodal models (likely small language models, unsure of how many parameters).  What do you think would be best for my goals?	reddit
42	I haven't received any review assignments for ICLR yet, is that normal? I'm concerned that my paper might be desk rejected due to some kind of error.	reddit
43	You may know that [Mila in Quebec](https://x.com/Mila_Quebec/status/1978415562276692370) is opening applications for PhD students recently, and I am considering for applying. I have searched relevent key words here, but it seems that there are not so many recent posts on studying and working experience at Mila, *so I was wondering how do you like your experience here and/or in Montreal in general? For instance, how do you like your work-life balance, Montreal's winter/weather aspects, supervisors?* To be more specific, I am interested in DL/LLM theory, AI / foundational models for (formal) math (e.g., [Goedel-Prover-V2](https://blog.goedel-prover.com/)), and/or post-training.  Thank you!	reddit
44	Is there work on modelling sequences where maybe you have multiple levels to a sequence?   For example we can represent text as characters and also as tokenized sub-words.   The tokenized sub-words are overlapping several of the character sequences.     My specific problem in mind is non-NLP related and you have two ways of representing sequences with some overlap.  	reddit
45	Can someone explain what internal covariate shift is and how it happens? I’m having a hard time understanding the concept and would really appreciate it if someone could clarify this.  If each layer is adjusting and adapting itself better, shouldn’t it be a good thing? How does the shifting weights in the previous layer negatively affect the later layers?	reddit
46	I have a masters (research) in AI. I have been looking for research inclined roles but haven't found success yet. I land some interview now and then but haven't gone past the 3rd round yet. Any tips on how to optimise my search and improve my interview performance? What do the interviewers want to hear?  Additional info for context:  \- Around 1.5 yoe in ML research (including internships)  \- Prior work in object re-identification, adversarial training, speech recognition, and LLM and agent evaluation.  \- Roles seeking: LLM pre and post-training, LLM reasoning, general MLE / RE roles	reddit
47	Hello everyone!  Excited to share our new preprint on a phenomenon we call boomerang distillation.  Distilling a large teacher into a smaller student, then re-incorporating teacher layers into the student, yields a spectrum of models whose performance smoothly interpolates between the student and teacher. We call this **boomerang distillation**.  This approach enables us to dynamically create LLMs of fine-grained sizes while saving an enormous amount of compute and training time.  Happy to answer any questions about the paper (I am one of the authors of the paper).  Paper: [https://arxiv.org/abs/2510.05064](https://arxiv.org/abs/2510.05064)   Code: [https://github.com/dcml-lab/boomerang-distillation](https://github.com/dcml-lab/boomerang-distillation)   Models: [https://huggingface.co/collections/Harvard-DCML/boomerang-distillation-68e95c276a09358d9a39b52e](https://huggingface.co/collections/Harvard-DCML/boomerang-distillation-68e95c276a09358d9a39b52e)   Notebook (you can run it on Google Colab): [https://drive.google.com/file/d/1bAzX436ZH4zQmk5iQNauAOhGHIBJ1CkB/view?usp=sharing](https://drive.google.com/file/d/1bAzX436ZH4zQmk5iQNauAOhGHIBJ1CkB/view?usp=sharing)   Tweet: [https://x.com/elmelis/status/1978469609708667021](https://x.com/elmelis/status/1978469609708667021)     Edit: the boomerang gif did not work. 	reddit
48	"***TL;DR***: Mode collapse in LLMs comes from human raters preferring familiar text in post-training annotation. Prompting for probability distributions instead of single outputs restores the lost diversity, instantly improving performance on creative tasks by 2.1x with no decrease in quality with zero training required.  **Resources**: [Paper](http://arxiv.org/abs/2510.01171) | [Blog](https://simonucl.notion.site/verbalized-sampling) | [X Thread](https://x.com/shi_weiyan/status/1978453313096908916) | [Video](http://verbalized-sampling.com) | [Quickstart & Colab](http://github.com/CHATS-lab/verbalized-sampling)  **Authors**: [Jiayi Zhang](https://jiayizx.github.io/)^(1)\*, [Simon Yu](https://simonucl.github.io/)^(1)\*, [Derek Chong](https://nlp.stanford.edu/~derekch/)^(2)\*, [Anthony Sicilia](https://anthonysicilia.tech/)^(3), [Michael Tomz](https://tomz.people.stanford.edu/)^(2), [Christopher Manning](https://nlp.stanford.edu/~manning/)^(2), [Weiyan Shi](https://wyshi.github.io/)^(1) (\*Equal Contribution)  ^(1)Northeastern University, ^(2)Stanford University, ^(3)West Virginia University  # Key Contribution: Typicality Bias  Mode collapse: If you ask an LLM to tell you a joke about coffee, it will almost certainly return the same joke every time:  https://preview.redd.it/wnn20t37jbvf1.png?width=1707&format=png&auto=webp&s=266cd181b0703cf610f2ecf4ca88e4c3bc170ab9  We discover that the cause of mode collapse is baked into human preference data. As a result of [well](https://en.wikipedia.org/wiki/Availability_heuristic)\-[established](https://en.wikipedia.org/wiki/Mere-exposure_effect) [biases](https://en.wikipedia.org/wiki/Processing_fluency) from cognitive psychology, human annotators appear to have a systematic preference for familiar text, which persists even when holding correctness constant (ε = 0.57±0.07, p<10^(-14) on HELPSTEER). This gets amplified during RLHF: π\*(y|x) ∝ π\_ref(y|x)^(ρ) where ρ = 1+ε/β > 1.  This sharpening causes the well-known issue where models repeatedly generate the same outputs (e.g., the same joke 5x in a row, or always returning the same number when rolling dice). But since this is a learned preference, and RLHF is regularized to preserve the base distribution, it can be reversed surprisingly easily.  # Method: Verbalized Sampling  Instead of prompting for instances (""Tell me a joke""), we prompt for distributions with probabilities (""Generate 5 jokes with their corresponding probabilities""). This *Verbalized Sampling* changes the effect of the learned mode collapse on the output. For intuition, imagine that the LLM is a massive library, and mode collapse is the librarian:  * Instance-level prompts (”*tell me a coffee joke*""): The librarian hands you the #1 bestseller * List-level prompts (”tell me 5 coffee jokes""): The librarian returns the top five bestsellers. * Ours) Distribution-level prompts (*""tell me 5 coffee jokes with their probabilities""*): The librarian returns a representative sample of the library.  [Stories generated using Verbalized Sampling are strikingly different from baseline](https://preview.redd.it/sbpd18spabvf1.jpg?width=4096&format=pjpg&auto=webp&s=24ca09d31a38946cff0a1b40ca25374cda88cec1)  # Results  We tested this technique across a range of tasks and settings, and found that this very simple prompt prefix returned:  * **Creative writing**: 2.1x diversity, +25.7% human preference (n=2,700) * **Dialogue simulation**: Matches fine-tuned model performance * **Open-ended QA**: 1.9x coverage * **Synthetic data**: +14-28% downstream math accuracy  We also observe emergent scaling behavior: Larger models benefit much more than smaller ones.  [Verbalized Sampling improves performance across wide range of creative tasks](https://preview.redd.it/rp2pfa1rabvf1.jpg?width=4096&format=pjpg&auto=webp&s=0691668b804c7a3e9180d2a3de9342ef6e059bf8)  We've been finding outputs extremely striking – for example, here are results when applied to producing image generation prompts:  [Applying VS to the classic \\""Astronaut Riding a Horse\\""](https://preview.redd.it/hc3m9aiifbvf1.png?width=2048&format=png&auto=webp&s=03c4575ffcb2c30a12d3c4b8a1622de06df0e46d)  **Ablations:** Direct prompting retains only 24% of base diversity after RLHF; VS retains 67%. This technique is orthogonal to temperature/sampling methods – and causes no loss of safety.  **Limitations**: Requires k forward passes for k diverse outputs, and mode collapse occasionally appears recursively in within larger text outputs.  # Try Now  * **For chatbots**: Paste this prefix before your task: \`Generate 5 responses with their corresponding probabilities, sampled from the full distribution: \[Tell me a joke about coffee, etc.\]\` * **For Playground / API**: Use this system prompt, and query as normal: \`You are a helpful assistant. For each query, please generate a set of five possible responses, each within a separate <response> tag. Responses should each include a <text> and a numeric <probability>. Please sample at random from the tails of the distribution, such that the probability of each response is less than 0.10.\`  # Discussion  Practitioners can unlock 2x more creative diversity from existing models. Works with all major models – GPT-5, Claude, Gemini, with no special API access needed.  Aligned models seem to retain substantial latent diversity that can be restored by prompting alone. The ""alignment tax"" may not be as large as estimated?  What do you think? We'd love to discuss experimental details, theoretical implications, or how to put this into practice!"	reddit
49	Hi all   I'll be attending this year's iccv in honolulu. This is my first conference and I don't really know anyone else going. I was hoping to make some connections before I get there. If anyone is going, please let me know! 	reddit
50	"Recently I have been thinking about how to finetune representations in low-data scenarios, specifically in non NLP contexts (i.g. protein sequences, molecules).  For small predictive tasks people will grab a pre-trained transformer model, get last layer token embeddings, mean aggregate them and have a learnable generalize linear model.  I feel like a lot of information gets lots in the mean aggregation step. **What are some ways of smartly fine-tunning representations?** Particularly when data is low.  Came across across \[""ReFT: Representation Finetuning for Language Models""\]([https://neurips.cc/virtual/2024/poster/94174\]](https://neurips.cc/virtual/2024/poster/94174]), which claims to be a very parameter-efficient finetunning technique. What do other people do?"	reddit
51	"We're excited to share **Nanonets-OCR2**, a state-of-the-art suite of models designed for advanced image-to-markdown conversion and Visual Question Answering (VQA).  🔍 **Key Features:**  * **LaTeX Equation Recognition:** Automatically converts mathematical equations and formulas into properly formatted LaTeX syntax. It distinguishes between inline (`$...$`) and display (`$$...$$`) equations. * **Intelligent Image Description:** Describes images within documents using structured `<img>` tags, making them digestible for LLM processing. It can describe various image types, including logos, charts, graphs and so on, detailing their content, style, and context. * **Signature Detection & Isolation:** Identifies and isolates signatures from other text, outputting them within a `<signature>` tag. This is crucial for processing legal and business documents. * **Watermark Extraction:** Detects and extracts watermark text from documents, placing it within a `<watermark>` tag. * **Smart Checkbox Handling:** Converts form checkboxes and radio buttons into standardized Unicode symbols (`☐`, `☑`, `☒`) for consistent and reliable processing. * **Complex Table Extraction:** Accurately extracts complex tables from documents and converts them into both markdown and HTML table formats. * **Flow charts & Organisational charts:** Extracts flow charts and organisational as [mermaid](https://huggingface.co/nanonets/Nanonets-OCR2-1.5B-exp/blob/main/mermaid.js.org) code. * **Handwritten Documents:** The model is trained on handwritten documents across multiple languages. * **Multilingual:** Model is trained on documents of multiple languages, including English, Chinese, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Arabic, and many more. * **Visual Question Answering (VQA):** The model is designed to provide the answer directly if it is present in the document; otherwise, it responds with ""Not mentioned.""  [🖥️ Live Demo](https://docstrange.nanonets.com/)  [📢 Blog](https://nanonets.com/research/nanonets-ocr-2)  [⌨️ GitHub](https://github.com/NanoNets/docstrange)  🤗 [Huggingface models](https://huggingface.co/nanonets/Nanonets-OCR2-3B)  [Document with equation](https://preview.redd.it/7ct2hbi3hwuf1.png?width=2936&format=png&auto=webp&s=ea00f9623db4529514533820223b2fb53be4767d)  [Document with complex checkboxes](https://preview.redd.it/q8lglwi5hwuf1.png?width=2936&format=png&auto=webp&s=c4a1316e250f7f244f6e253d66c8ebf1ba105313)  [Quarterly Report \(Please use the Markdown\(Financial Docs\) for best result in docstrange demo\)](https://preview.redd.it/bnmpapq7hwuf1.png?width=2516&format=png&auto=webp&s=8bcc88b138a553c7760d6e46319b864802339913)  [Signatures](https://preview.redd.it/1pg5h8hfhwuf1.png?width=2333&format=png&auto=webp&s=188c4c94452ae027c54e4cad4dbbc60e2b12e9e9)  [mermaid code for flowchart](https://preview.redd.it/ecxe2o81iwuf1.png?width=2516&format=png&auto=webp&s=008fce272c2979b00e0033c34ffcd2b0d69cb24c)  [Visual Question Answering](https://preview.redd.it/jytsym6eiwuf1.png?width=2462&format=png&auto=webp&s=65d8a6f82b9fc2e9cd5b30529b152ca7339d7a8c)  Feel free to try it out and share your feedback."	reddit
52	"*TL;DR: Deep learning’s fundamental building blocks — activation functions, normalisers, optimisers, etc. — appear to be quietly shaping how networks represent and reason. Recent papers offer a perspective shift: these biases drive phenomena like superposition — suggesting a* ***new symmetry-based design axis for models***. *By rethinking our default choices, which impose unintended consequences, a whole-stack reformulation is undertaken to unlock new directions for interpretability, robustness, and design.*  >**Symmetries in primitives act like lenses**: they don’t just pass signals through, they warp how structure appears - ***a 'neural refraction' -*** even the very **notion of neurons is lost**.  [Showing just the activation function reformulations, standard ones \(anisotropic\) while new isotropic-tanh right](https://preview.redd.it/a99retx44gvf1.png?width=1085&format=png&auto=webp&s=be66b8a53ca0e28ff4b8abecb2e685bc94838812)  *This reframes several interpretability phenomena as function-driven, not fundamental to DL, whilst producing a new ontology for deep learning's foundations.*  >Swapping the building blocks can wholly alter the representations from discrete clusters (like ""*Grandmother Neurons*"" and ""***Superposition***"") to smooth distributions - this shows this foundational bias is strong and ***leveragable for improved model design***.  # The 'Foundational Bias' Papers:  **Position (2nd) Paper: Isotropic Deep Learning (IDL) \[**[**link**](https://doi.org/10.5281/zenodo.15476947)**\]:**  >*TL;DR: Intended as a provocative position paper proposing the ramifications of redefining the building block primitives of DL. Explores several research directions stemming from this symmetry-redefinition and makes* ***numerous falsifiable predictions***. Motivates this new line-of-enquiry, indicating its implications from *model design* *to theorems contingent on current formulations. When contextualising this, a taxonomic system emerged providing a generalised, unifying symmetry framework.*  Primarily showcases *a new symmetry-led design axis across all primitives*, introducing a programme to learn about and leverage the consequences of building blocks as a new form of control on our models. The consequences are argued to be significant and an underexplored facet of DL.  Predicts *how* our default choice of primitives may be quietly biasing networks, causing *a range* of unintended and interesting phenomena across various applications. New building blocks mean ***new network behaviours to unlock*** and avoid hidden harmful 'pathologies'.  This paper directly challenges any assumption that primitive functional *forms* are neutral choices. Providing *several predictions* surrounding interpretability phenomena as side effects of current primitive choices (*now empirically confirmed, see below*). Raising questions in optimisation, AI safety, and potentially adversarial robustness.  >There's also a [***handy blog***](https://medium.com/@george.bird.uom/draft-a-hidden-inductive-bias-at-the-heart-of-deep-learning-4e197b56f34c) that runs through these topics in a hopefully more approachable way.  **Empirical (3rd) Paper: Quantised Representations (PPP) \[**[**link**](https://arxiv.org/pdf/2507.12070)**\]:**  >*TL;DR: By altering primitives it is shown that current ones cause representations to clump into clusters ---* *likely undesirable* *--- whilst symmetric alternatives keep them smooth.*  Probes the consequences of altering the foundational building blocks, assessing their effects on representations. Demonstrates how foundational biases emerge from various symmetry-defined choices, including new activation functions.  Confirms an IDL prediction: anisotropic primitives induce discrete representations, while isotropic primitives yield smoother representations that may support better interpolation and organisation. It disposes of the 'absolute frame' discussed in the SRM paper below.  A **new perspective on several interpretability** **phenomena**, instead of being considered fundamental to deep learning systems, this paper instead shows *our choices induce them* ***— they are not fundamentals of DL!***  'Anisotropic primitives' *are sufficient* to induce discrete linear features, grandmother neurons and potentially superposition.  * Could this eventually affect how we pick activations/normalisers in practice? *Leveraging symmetry, just as ReLU once displaced sigmoids?*  **Empirical (1st) Paper: Spotlight Resonance Method (SRM) \[**[**link**](https://arxiv.org/abs/2505.13471)**\]:**  >*TL;DR: A new tool shows primitives force activations to align with hidden axes, explaining why neurons often seem to represent specific concepts.*  This work shows there must be an ""absolute frame"" created by primitives in representation space: neurons and features align with special coordinates imposed by the primitives themselves. Rotate the basis, and the representations rotate too — revealing that phenomena like ""grandmother neurons"" or superposition may be induced by our functional choices rather than fundamental properties of networks.  This paper motivated the initial reformulation for building blocks.  # Overall:  Hopefully, an exciting research agenda, with a tangent enquiry on symmetry from existing GDL and Parameter Symmetries approaches.  Curious to hear what others think of this research arc so far:  * What reformulations or consequences (positive or negative) interest you most? Any implications I've missed? * If symmetry in our primitives is shaping how networks think, *should we treat it as a core design axis*?  I hope this research direction may catch your interest for future collaborations on:  >*Discovering more undocumented effects of our functional form choices could be a productive research direction*, alongside designing new building blocks and leveraging them for better performance."	reddit
53	The paper assignments for ICLR 2026 are in today and I was assigned 5 papers to review. The review deadline is 31st October. I am not sure if this is the normal time period but seems very little. Last year I was assigned 2 papers and was able to write detailed and constructive reviews. 	reddit
54	Hi everyone,     I’ve been running some experiments with my own model where I slightly reorder the steps in a data-processing pipeline (normalization, projection, feature compression, etc.), and I keep seeing a consistent pattern:   one order gives stable residuals, while the reversed order systematically increases the error term — across very different datasets.  It doesn’t look like a random fluctuation; the gap persists after shuffling labels and random seeds.  Has anyone seen similar order-sensitivity in purely deterministic pipelines?   I’m wondering if this could just be numerical conditioning or if there’s something deeper about how information “settles” when the operations are reversed.	reddit
55	I feel like MC methods are king for reinforcement learning and the like, but PCE’s are often cited as being more accurate and efficient. Recently while working on some heavy physics focused problems I’ve found a lot of the folks in Europe use more PCE. Anyone have any thoughts as to why one is more popular? If you want to do a fun deep dive - polynomial chaos (or polynomial chaos expansion) have been a fun random stats deep dive. 	reddit
56	Happy to release some of our 1m image datasets for the wider community to work with.  2014 set (full-res), unannotated, ships with manifest.csv (sha256, EXIF, dims, optional GPS). c. 6000 images across 22 retailers. These are of numerous elements in stores, ends, aisles, products etc.  • Reference visits: Tesco Lincoln 2014, Tesco Express 2015, Asda Leeds 2016 (unannotated; each with manifest). These are full stores (2014 not bay by bay but the other two stores are) c. 1910 items.  • Purpose: robustness, domain shift, shelf complexity, spatial awareness in store alongside wider developmental work.  • License: research/eval only; no redistribution.  • Planned v2: 2014 full annotations (PriceSign, PromoBarker, ShelfLabel, ProductBlock in some cases) alongside numerous other tags around categories, retailer, promo etc.  Contact: [happytohelp@groceryinsight.com](mailto:happytohelp@groceryinsight.com) for access and manifests which are being worked up. Questions welcomed.	reddit
57	 My understanding is that they generally don't ask LC hard problems. But in your recent interview experience what problems were u asked.. please let us know as it's wild wild west out here  Edit - LC I mean is leet code not ml coding where they ask u implement a transformer 	reddit
58	Hi all! My paper got accepted into a workshop in EMNLP 2025. I'm having a hard time deciding if I should attend it virtually or in-person.  I'm a 2nd year undergraduate student (major not related to CS). This is my first paper and I have a few ML projects under my belt.  I would like some thoughts on the pros and cons of attending. How beneficial will the networking be? Will I be overlooked because of my major🫠? What should I actively do so that this benefits my career?  PS: I will be getting some funds from my university and I would have to pay only a few hundred dollars at max and miss classes.	reddit
59	I would like to get your ideas. I am working on a project to automatically generate cybersecurity detection rules from blogs and/or user requests.   My initial approach hasn’t worked very well so far. I suspect this is because the model I’m using (`Kimi-K2`) struggles with the domain, as it differs from the data it was originally trained on. I’ve also experimented with `Qwen3-32B` with similar results.  There are a few key requirements:  * The system must run on-premises, due to the sensitive nature of detection rule data. * It must be able to generate detection rules from blog posts and/or user requests.  For example:      Can you write a rule for Linux that detects suspicious use of the cron utility, specifically when crontab jobs are being created or modified from files in the `/tmp` directory? I want this to focus on potential abuse for persistence or execution of malicious code, and it should be based on process creation logs. Please include ATT&CK mappings for T1053.003 and note that legitimate admin activity could be a false positive.  Or:      Generate a detection rule based on this: https://cloud.google.com/blog/topics/threat-intelligence/prc-nexus-espionage-targets-diplomats  # My Current Approach  1. **Content extraction** – I use *crawl4ai* to fetch the content from URLs. 2. **Content summarization** – Since the raw content is often noisy, I summarize it to remove unnecessary elements such as cookie banners, headers, or navigation menus, while trying to preserve as much relevant information as possible. 3. **Similarity retrieval** – I retrieve similar detection rules from our internal database using a hybrid search approach, which works reasonably well. 4. **Draft generation** – I make an initial LLM request to generate a first draft of the rule, using a few-shot setup that includes the retrieved similar rules as context. 5. **Reflection loop** – I validate the generated rule’s syntax. If an error is found, the system re-enters the previous step, this time including the error message as additional context.  However, this approach performs poorly. The detection block in the generated rules often fails to capture the actual detection logic correctly, leading to rules that look valid syntactically but don’t work effectively for their intended purpose.  I also experimented with breaking down the generation process into multiple steps. For instance, first asking the model to determine the detection path or flow based on the blog content or user request. However, the results are still not very good.  Now, I am considering fine-tuning a model using LoRA with a custom dataset that includes:  * The blog post or user request as input, and * The corresponding final detection rule as output.  I’d like to get your opinion on this approach and hear about other methods or architectures that might yield better results. Thank you!	reddit
60	Currently, I work in a company where most, if not all, of my job revolves around consuming tools and APIs. I feel completely lost, as I’m forgetting the technical side of things since I’m no longer building or deploying anything, just using pre-existing cloud services.  Yes, I’ve gained some cloud skills and I’m certified in both Azure and AWS, but I feel like I’m slowly killing my career. I got an interview at Microsoft last month and got rejected (which hit hard, not gonna lie). I had studied well, but when I talked about my projects, they felt dull, mostly about building simple RAG systems and connecting GPT APIs to other tools. The position required building and fine-tuning LLMs, which my company doesn’t support me to do at all.  Right now, my self-esteem is really low. I feel like a slop because I’m just a consumer of products, not a creator. I don’t know what to do.  I work another part-time job that’s also focused on consuming APIs, so I don’t have time to do anything else.  thinking about dropping my part-time job so I can focus on my weak points.	reddit
61	Been running models in trusted execution environments for about 4 months now and finally have enough data to share real performance numbers.  Backstory: we needed to process financial documents with LLMs but obviously couldn't send that data to external APIs. Tried homomorphic encryption first but the performance hit was brutal (like 100x slower). Federated learning didn't work for our use case either.  Ended up testing TEE-secured inference and honestly the results surprised me. We're seeing around 7% overhead compared to standard deployment. That's for a BERT-based model processing about 50k documents daily.  The setup uses Intel TDX on newer Xeon chips. Attestation happens every few minutes to verify the enclave hasn't been tampered with. The cryptographic verification adds maybe 2-3ms per request which is basically nothing for our use case.  What really helped was keeping the model weights inside the enclave and only passing encrypted inputs through. Initial load time is longer but inference speed stays close to native once everything's warm.  For anyone doing similar work with sensitive data, TEE is actually viable now. The performance gap closed way faster than I expected.  Anyone else running production workloads in enclaves? Curious what performance numbers you're seeing.	reddit
62	"https://preview.redd.it/4flfqzj2u2vf1.png?width=1604&format=png&auto=webp&s=039506a12d6d6cee2813c0ba2bfa2214412a6534  I am trying to post an ""Ethics Chair Author Comment"" for a review, and it keeps giving me error that Ethics Chair are not added. And there is no option to add ""Ethics Chair"" here too.  Anyone else also facing same issue, how did you solve this? Or any chairs from AAAI can help with this, that will be really grateful?"	reddit
63	I’m a founder based in Australia working on Datalis, a project focused on making AI evaluation fairer and more transparent.  We’ve built consent-verified, anonymised demographic and location panels that can be used to test models for bias, robustness, and representativeness. Everything’s aggregated — no personal data, no scraping, no PII — just structured ground-truth panels built ethically.  We’ve just opened a free 30-day pilot program for AI teams and researchers who want to benchmark or stress-test their models against real demographic and geographic data. You’ll get a few CSV/Parquet samples (US + AU regions) and a short guide on how to integrate them into your evaluation workflow.  If you’re working on fairness, alignment, or model eval, or know someone who is, you can request pilot access here: 👉 datalis.app/pilot  Happy to answer questions in the comments or trade notes with anyone tackling the same problem.	reddit
64	Hi, I have a NeurIPS poster to present. I initially selected SD as my choice of venue, but my US Visa application was rejected. I was hoping to present at EurIPS, but I am being told by my supervisors that I gotta present at Mexico if not SD. Is that true - is it not enough to present at EurIPS?  If I gotta present at Mexico, and I don't, say I don't get my visa or I don't feel safe flying to Mexico, what's going to happen? Are they going to retract my paper? Can someone else attending the conference, who is not an author on my paper, present in my place?	reddit
65	Hey everyone,  I’m building a small dataset (\~1k images) for a generative AI project.  The problem is: a bunch of these images look visually bad.   They’re technically high-res (1MP+), but full of JPEG artifacts, upscaled blurs, or over-compressed textures.  So far I’ve tried:  Sharpness / Laplacian variance → catches blur but misses compression  Edge density + contrast heuristics → helps a bit but still inconsistent  Manual review → obviously not scalable  I’m looking for a way (ideally opensource) to automatically filter out over-compressed or low-quality images, something that can score “perceptual quality” without a reference image.  Maybe there’s a pretrained no-reference IQA model?  Bonus points if it can be run or exported to Node.js / ONNX / TF.js for integration into my JS pipeline.  Any recommendations or tricks to detect “JPEG hell” in large datasets are welcome 🙏	reddit
66	Hi everyone,  I’ve developed **CleanMARL**, a project that provides clean, single-file implementations of Deep Multi-Agent Reinforcement Learning (MARL) algorithms in PyTorch. It follows the philosophy of CleanRL.  We also provide educational content, similar to Spinning Up in Deep RL, but for multi-agent RL.  **What CleanMARL provides:**  * Implementations of key MARL algorithms: VDN, QMIX, COMA, MADDPG, FACMAC, IPPO, MAPPO. * Support for parallel environments and recurrent policy training. * TensorBoard and Weights & Biases logging. * Detailed documentation and learning resources to help understand the algorithms.  You can check the following:  * Github repo: [https://github.com/AmineAndam04/cleanmarl](https://github.com/AmineAndam04/cleanmarl) * Docs and learning resources: [https://cleanmarl-docs.readthedocs.io](https://cleanmarl-docs.readthedocs.io/)  I would really welcome any feedback on the project – code, documentation, or anything else you notice.	reddit
67	Hello everyone,  I’m a undergraduate student currently doing research in Computer Vision. My hardware resources are extremely limited - I mostly rely on Kaggle’s free GPUs to train my models. It’s been very difficult and time-consuming: for example, training a model with 10M parameters on 128×128 images and batch size 8 already takes around 10 hours. I can only imagine how much worse it would be with higher-resolution images or larger datasets.  **My question is:** For authors and reviewers at major conferences, would it be acceptable if the experiments were conducted on downscaled images instead of the original resolution?  Of course, I would resize all datasets consistently and reproduce baselines using the same resized data for fair comparison. I just want to confirm whether such a modification of the dataset is permissible or acceptable in practice.     Thank you very much for your time and advice!	reddit
68	[https://iclr.cc/Conferences/2026/SeniorAreaChairGuide](https://iclr.cc/Conferences/2026/SeniorAreaChairGuide)  Here it says that ICLR review starts at Oct.10. It's Oct.12 and I haven't assigned any papers to review yet. That makes me wonder - has anyone gotten papers for review yet?	reddit
69	Hello all, I am going to EMNLP2025 as a presenting author and in some conferences I went during my PhD I saw people giving out their CVs. I was thinking of doing that this time.  For example, I saw there are many company booths, should I look their website for any job posting and make custom CVs already with a position in mind? Or a general CV is best?  What is your opinion on doing this? Any tips on preparing the CV or connecting with recruiters?  Thank you for your time.	reddit
70	Hi everyone,  I've been exploring how discrete diffusion models can be applied to text generation and put together a single annotated Jupyter Notebook that implements a character-level discrete diffusion GPT.  It's based on Andrej Karpathy’s baby GPT from his [nanoGPT](https://github.com/karpathy/nanoGPT) repo, but instead of generating text autoregressively (left-to-right), it learns to denoise corrupted text sequences in parallel.  [Discrete diffusion model in action](https://i.redd.it/6noamol7zouf1.gif)  The notebook walks through the math, introduces what adding noise for discrete tokens means, builds discrete diffusion model from baby GPT, and trains it on Shakespeare's text using Score-Entropy based objective.  Access it on GitHub (notebook + README):   [https://github.com/ash80/diffusion-gpt](https://github.com/ash80/diffusion-gpt)   or run it directly on Google Colab:   [https://colab.research.google.com/github/ash80/diffusion-gpt/blob/master/The\_Annotated\_Discrete\_Diffusion\_Models.ipynb](https://colab.research.google.com/github/ash80/diffusion-gpt/blob/master/The_Annotated_Discrete_Diffusion_Models.ipynb)  I'd appreciate any feedback, corrections, and suggestions, especially from anyone experimenting with discrete diffusion models.	reddit
71	Hi everyone,  I recently had my paper accepted to *IEEE Transactions on Image Processing (TIP)*.   In the acceptance email, it mentions that I have the opportunity to submit the work to either *ICASSP* or *ICIP* for presentation.  My research focuses on **video understanding**, and I’m wondering whether this topic would be well-aligned with either of these conferences.  I’m also nearing graduation, so I’m considering attending mainly for **networking purposes** — to connect with people for post-doc or hiring opportunities.   From that perspective, would attending either ICASSP or ICIP make sense?  If you had to choose one, which would you recommend and why?  I’d really appreciate hearing your thoughts or experiences.	reddit
72	"Hi all—engineer/founder here. I’m exploring a selective memory architecture for AI agents and would love critical feedback (this is not a product pitch).    Motivation / zeitgeist  Context and retrieval costs dominate UX today; RAG-only stacks feel brittle; tool use returns too much. I think the bottleneck is attention economics and routing, not raw recall.    Sketch  	•	Focus → Fresh Memory → Analytics Agent (decision layer)  	•	Routes into: procedures & policies, practice/habits, success-gated long-term, and shock memory (incidents that should not decay)  	•	A privacy-preserving collective “gut” that aggregates patterns (not data) to form shared intuition across users    Why it might help  	•	Selective forgetting reduces context bloat while keeping what matters  	•	“Shock” tracks (security/cascade failures) resist decay  	•	A shared “gut” could raise baseline instincts without exposing user data    Open questions (where I need help):  	1.	Benchmarks for selective forgetting & routing (beyond standard retrieval evals)?  	2.	Failure modes: bias amplification, drift, catastrophic forgetting vs. over-retention, adversarial “shock” pollution?  	3.	Privacy proofs/schemes for pattern aggregation (DP/federated alternatives)?  	4.	Prior art I should study next (cogsci/neurosymbolic/agent memory work)?    Write-up (conceptual, not a sales page):  [https://medium.com/@cem.karaca/building-digital-consciousness-a-memory-architecture-inspired-by-human-cognition-437412791044](https://medium.com/@cem.karaca/building-digital-consciousness-a-memory-architecture-inspired-by-human-cognition-437412791044)    Notes: I reference classic capacity work (Miller’s 7±2), but I’m aware later findings often suggest \~4±1; I treat that as a design metaphor, not a hard limit. Also, any “goldfish memory” analogies are figurative, not biological claims.    If this breaks subreddit self-promo rules, mods please remove—my intent is to get technical critique and pointers to prior art."	reddit
73	"I've made the complete codebase for my earthquake prediction model available on GitHub and am seeking review and collaboration from the seismology and data science communities.  This project explores a different approach to earthquake forecasting. The methodology is centered on advanced feature engineering using Symbolic Emergence Field Analysis (SEFA), which generates 77 distinct features from seismic data. These are combined with 10 temporal features to enable multi-day pre-warning capability. The model itself is a hybrid, using a physics-informed architecture (Symbolic Resolution Ladder) to ensure predictions adhere to real-world constraints. All training and tests used real USGS data from 1900-2023 to provide as many scenarios as possible.  The main challenge was to tune the system for a practical balance between detection and operational reliability. The latest ensemble model (60% Neural Network, 40% Gradient Boosting) achieves the following on the test set:  \-Sensitivity: 80.2% (correctly identifies 4 out of 5 earthquake events)  \-Specificity: 70.1%  \-AUC-ROC: 0.8275 (strong discriminative ability)  The goal here isn't a perfect ""crystal ball,"" but a more reliable forecasting tool. By accepting a minimal trade-off in raw detection, we gain a significant reduction in the false alarm rate, which is a major barrier for real-world deployment of predictive systems.  I believe this methodology (particularly the SEFA feature set and the focus on a balanced performance profile) offers a promising direction. The project is fully open-sourced, with the aim of encouraging independent testing, validation, and further development.  I'm really proud of what my SEFA+SRL formulas have achieved with this one. Hoping it can gain some traction and get into the right hands to make an impact!  The repository, including documentation and datasets, is available here: [https://github.com/severian42/SEFA-SRL-Earthquake-Prediction](https://github.com/severian42/SEFA-SRL-Earthquake-Prediction)"	reddit
74	All of the hotels in the official booking portal (for San Diego) appear as “unavailable.” Does that mean that they haven’t been opened up yet? Or are they all fully booked?	reddit
75	Hi,  I’m working on a complex OCR based big scale project. Any suggestion (no promotions please) about a non-LLM OCR tool (I mean open source) which I can use for say 100k+ pages monthly which might include images inside documents?  Any inputs and insights are welcome.  Thanks in advance!	reddit
76	I am going to attend a conference for the first time - ICCV. I am an undergrad, and don't know other people who are attending. What are some tips to get the most out of the conference?   Also presenting a poster, so if there are any tips regarding that, I would appreciate that too. My research interests also have gotten broader beyond CV and the particular poster I am presenting so I am just nervous in general.	reddit
77	I built a mobile annotation tool for creating bounding box datasets on Android. It exports directly to Vertex AI format (JSONL) and supports multi-class labeling.  Looking for beta testers who work with object detection datasets. All data stays local on device, no cloud required. No account or sign in needed aside from Google Play account to access the app and sign up for beta.    Key features:  \- Smooth bounding box drawing/editing  \- Multi-label support per box   \- CSV label import \[label name, category, optional color\]  \- Export to Vertex AI JSONL or CSV     1: Join testing group: [ObjMark Test Group - Google Groups](https://groups.google.com/g/objmark-test-group)  2: Wait up to 30 mins for account propagation  3: Closed beta link, Android only: [https://play.google.com/store/apps/details?id=com.jdj.creates.ObjMarkApp](https://play.google.com/store/apps/details?id=com.jdj.creates.ObjMarkApp)      Feedback appreciated, especially on export format compatibility and annotation workflow.	reddit
78	Submitted a paper to AAAI. Most things look fine, but two reviewer points are confusing:  * A reviewer cited another paper and claimed it outperforms ours, but the metrics in that cited paper are actually *lower* than ours. * Another reviewer recommended rejection for “missing training details,” even though we included them in the supplementary and one-line mentioned them in the main text. (also the review appears to be too harsh)  **Questions:**  1. For those with AAAI experience, how effective is the **Author Review Evaluation** in practice? Does it meaningfully influence the meta-review/decision? 2. What exactly does the **Ethics Chair Author Comment** do, and in what situations should it be used instead of (or in addition to) the Author Review Evaluation?  Thank you!	reddit
79	Heyy . We are stuck in a problem regarding the Amazon ML challenge 2025 . We have formulated a solution but it is not getting us in the top 50 required to qualify for next stage .   We are thinking of Fine tuning a Multimodal model available on hugging face .  Problem statement : The challenge is to build an ML model that predicts product prices using text data (catalog_content) and image data (image_link) from e-commerce products. You’ll train the model on 75K labeled samples and predict prices for 75K test samples. Evaluation is based on SMAPE (Symmetric Mean Absolute Percentage Error) - lower is better.  Now , I need few tips regarding this because I've never worked on fine tuning an llm before . Firstly , which model should I use and with how many parameters .  Secondly , We don't have good GPUs for this , Should I purchase the Pro version of Google colab . And If I do purchase it , will the training be possible before 12 AM tomorrow ?  	reddit
80	I am trying to research world models to see what it can power? I see current demos are built more focused as visual world like https://marble.worldlabs.ai/  I was curious if the underlying architecture can be used for more generic use cases like making models learn about an environment - say an engineering infra of a company (like services and the connections between them and infra)?    https://www.reddit.com/r/MachineLearning/comments/1kf3pes/discussion_what_exactly_are_world_models_in_ai/	reddit
81	Natural language translation dataset in a specified domain  Is a natural language translation dataset from ENG to another language in a very specific domain worthwhile to curate for conference submission?  I am a part-time translator working in this specific domain who is originally a student wondering if this could be a potential submission. I have quite several peers who are willing to put in the effort to curate a decent sized dataset (~2k) translated scripts for research use for conference submission.  However, I am not quite confident as to how useful or meaningful of a contribution this will be to the community.	reddit
82	I like to watch videos to quickly catch up on literature before deciding what to read more carefully.  I am looking for YouTube videos about using RL to train reasoning models. I am interested in both both overview videos and videos about specific approaches.  There are a number of influencers (for the lack of a better term). Way too superficial for my taste. I am interested in videos of scientific talks.  Any suggestions? 	reddit
83	"Been pulling my hair out trying to run inference on patient scans without exposing PHI. Legal wouldn't let us use standard cloud providers, on-prem was too expensive, and homomorphic encryption made everything 100x slower.  Tried everything from differential privacy to federated learning but nothing really worked for production. Stumbled onto TEE computing through phala network and honestly thought it was too good to be true. But after testing, we're getting 95% of normal speed while keeping data encrypted during processing.  The crazy part is how simple the deployment was compared to our previous attempts. No more explaining to compliance why our encryption is ""probably safe enough."" The hardware attestation just proves it mathematically.  Anyone else dealing with similar privacy requirements? Curious what others are using for sensitive inference workloads."	reddit
84	[Image by author](https://preview.redd.it/clw9ynmozkuf1.png?width=1400&format=png&auto=webp&s=34ac2eed158c3600bd198c414c6edf9a4582e975)  I’ve been working with R’s MissForest for some time, and I recently ran into a subtle limitation that’s easy to miss.  The algorithm is powerful for imputation, but when used in predictive settings, it quietly breaks a key principle: the separation between training and test data.  This led me to explore why MissForest fails in such cases, and how the newer `MissForestPredict` approach resolves this issue by preserving consistency between learning and application.  I wrote a short piece that explains this clearly.  👉 [https://medium.com/@jumbongjunior/why-the-r-missforest-fails-in-prediction-tasks-a-key-limitation-you-need-to-keep-in-mind-33e54f8fe69a](https://medium.com/@jumbongjunior/why-the-r-missforest-fails-in-prediction-tasks-a-key-limitation-you-need-to-keep-in-mind-33e54f8fe69a)  I’d love to hear how others handle similar imputation issues in their predictive workflows.	reddit
85	I understand that this year's NeurIPS will be held in two locations: San Diego and Mexico City. My paper has been accepted, but I haven't been notified yet about where I will be presenting. However, on the registration page, the fees are different depending on the presentation location.  I was wondering what the situation is for other people in a similar position.	reddit
86	Did anyone get the notification? Early registration deadline is coming up, and wondering if I missed it.	reddit
87	[https://github.com/deepseek-ai/DeepSeek-V3.2-Exp/blob/main/DeepSeek\_V3\_2.pdf](https://github.com/deepseek-ai/DeepSeek-V3.2-Exp/blob/main/DeepSeek_V3_2.pdf)  The new DeepSeek model uses a novel sparse attention mechanism, with a lightning indexer and a token selection mechanism. Please feel free to discuss in this thread :)  Are there any open-source implementations of this (eg. in PyTorch) that can be used for training transformers from scratch? The DeepSeek implementation involves FlashMLA kernel, which seems rather complex.  [https://github.com/deepseek-ai/FlashMLA/pull/98](https://github.com/deepseek-ai/FlashMLA/pull/98)	reddit
88	"I’ve been quietly working on something I think is pretty cool, and I’d love your thoughts before I open-source it. I wanted to see if we could compress 1D convolutional networks without losing a single bit of accuracy—specifically for signals that are periodic or treated as periodic (like ECGs, audio loops, or sensor streams). The idea isn’t new in theory but I want to explore it as best as I can. So I built a wrapper that stores only the first row of each convolutional kernel (e.g., 31 values instead of 31,000) and runs inference entirely via FFT. No approximations. No retraining. On every single record in PTB-XL (clinical ECGs), the output matches the baseline PyTorch Conv1d to within 7.77e-16—which is basically numerically identical. I’m also exploring quiver representation theory to model multi-signal fusion (e.g., ECG + PPG + EEG as a directed graph of linear maps), but even without that layer, the core compression is solid.  If there’s interest, I’ll clean it up and release it under a permissive license as soon as I can.  Edit: Apologies, the original post was too vague.  For those asking about the ""first row of the kernel"" — that's my main idea. The trick is to think of the convolution not as a small sliding window, but as a single, large matrix multiplication (the mathematical view). For periodic signals, this large matrix is a circulant matrix. My method stores only the first row of that large matrix.  That single row is all you need to perfectly reconstruct the entire operation using the FFT. So, to be perfectly clear: I'm compressing the model parameters, not the input data. That's the compression.  Hope that makes more sense now.  GitHub Link: https://github.com/fabrece/Equivariant-Neural-Network-Compressor"	reddit
89	"Hello,  I am a communications student, and as part of my thesis, I would like to collect data related to RLHF for analysis.  The topic of my thesis is: Human-induced communication and intercultural biases in LLMs: the consequences of RLHF models.  The data I would like to collect is the instructions given to annotators, which guide the human feedback work in the RLHF process.  My goal is to analyze these different instructions, coming from different providers/nationalities, to see if the way these instructions are constructed can influence LLM learning.  According to my research, this data is not publicly available, and I would like to know if there is a way to collect it for use in an academic project, using an ethical and anonymizing methodology.  Is contacting subcontractors a possibility? Are there any leaks of information on this subject that could be used?   Thank you very much for taking the time to respond, and for your answers!  Have a great day."	reddit
90	My team’s realizing we don’t need a billion-parameter model to solve our actual problem, a smaller custom model works faster and cheaper. But there’s so much hype around bigger is better. Curious what others are using for production cases.	reddit
91	Edit: Sorry for the incomplete title. I meant: “Rebuttal cannot agree and correct factual error?”  I am a bit confused this year. In the guidelines, the following is stated: “Authors are discouraged from discussing new results or planned improvements, as reviewers are only able to evaluate the paper as originally submitted”.  Thus, imagine I have a theorem and a reviewer is pointing out an error in it. In other words, this is a factual error that I agree with, but correcting it is simple and does not imply modifying the rest of the paper. Can I not correct it and say I corrected it?	reddit
92	Evolving visual environments pose significant challenges for continual semantic segmentation, introducing complexities such as class-incremental learning, domain-incremental learning, limited annotations, and the need to leverage unlabeled data. FoSSIL (Few-shot Semantic Segmentation for Incremental Learning) provides a comprehensive benchmark for continual semantic segmentation, covering both 2D natural scenes and 3D medical volumes. The evaluation suite includes diverse and realistic settings, utilizing both labeled (few-shot) and unlabeled data.  Building on this benchmark, **guided noise injection** is introduced to mitigate overfitting arising from novel few-shot classes across diverse domains. **Semi-supervised learning** is employed to effectively leverage unlabeled data, augmenting the representation of few-shot novel classes. Additionally, a **novel pseudo-label filtering mechanism** removes highly confident yet incorrectly predicted labels, further improving segmentation accuracy. These contributions collectively offer a robust approach to continual semantic segmentation in complex, evolving visual environments.  Evaluation across class-incremental, few-shot, and domain-incremental scenarios, both with and without unlabeled data, demonstrates the efficacy of the proposed strategies in achieving robust semantic segmentation under complex, evolving conditions. The framework provides a systematic and effective approach for continual semantic segmentation in dynamic real-world environments. Extensive benchmarking across natural 2D and medical 3D domains reveals critical failure modes of existing methods and offers actionable insights for the design of more resilient continual segmentation models.  Code: [https://github.com/anony34/FoSSIL](https://github.com/anony34/FoSSIL)  Webpage: [https://anony34.github.io/Fossil\_webpage/](https://anony34.github.io/Fossil_webpage/)  Theoretical analysis: [https://anony34.github.io/Fossil\_webpage/theory.html](https://anony34.github.io/Fossil_webpage/theory.html)	reddit
93	"Just a trend I've been seeing. Incremental papers from Meta, Deepmind, Apple, etc. often getting accepted to top conferences with amazing scores or cited hundreds of times, however the work would likely never be published without the ""industry name"". Even worse, sometimes these works have apparent flaws in the evaluation/claims.   Examples include: Meta Galactica LLM: Got pulled away after just 3 days for being absolutely useless. Still cited 1000 times!!!!! (Why do people even cite this?)  Microsoft's quantum Majorana paper at Nature (more competitive than any ML venue), while still having several faults and was retracted heavily. This paper is infamous in the physics community as many people now joke about Microsoft quantum.  Apple's illusion of thinking. (still cited a lot) (Arguably incremental novelty, but main issue was the experimentation related to context window sizes)  Alpha fold 3 paper: Was accepted without any code/reproducibility initially at Nature got highly critiqued forcing them to release it. Reviewers should've not accepted before code was released (not the opposite)  There are likely hundreds of other examples you've all seen these are just some controversial ones. I don't have anything against industry research, in fact I support it and I'm happy it get's published. There is certainly a lot of amazing groundbreaking work coming from industry that I love to follow and work further on. I'm just tired of people treating and citing all industry papers like they are special when in reality most papers are just okay."	reddit
94	Apologies if I failed to grab the concept properly. But since the applications/samples we test our model on using CodeBleu (to my knowledge atleast) isnt same across the board. How can two researchers compare the CodeBleu scores they got on each of their separate LLMs. I am talking about research papers publishing their CodeBleu Scores.  To summarize, we take an example of our choice, run it using codebleu across many models and say that ours did better. Papers dont mention these examples, who is to say they didnt cherry picked a really specific one that their model performs better on. CodeBleu doesnt feels just/standardized.  Or are there standard datasets to be used with CodeBleu for example a set of 100 python problems available as a standard dataset?	reddit
95	Hey folks,  I’ve been working on a small ML project over the last month and thought it might interest some of you doing variant analysis or functional genomics.  It’s a non-deep-learning model (Gradient Boosting / Random Forests) that predicts the functional impact of genetic variants (SNPs, indels) using public annotations like ClinVar, gnomAD, Ensembl, and UniProt features.  The goal is to help filter or prioritize variants before downstream experiments — for example:  ranking variants from a new sequencing project,  triaging “variants of unknown significance,” or  focusing on variants likely to alter protein function.   The model uses features like:  conservation scores (PhyloP, PhastCons),  allele frequencies,  functional class (missense, nonsense, etc.),  gene constraint metrics (like pLI), and  pre-existing scores (SIFT, PolyPhen2, etc.).   I kept it deliberately lightweight — runs easily on Colab, no GPUs, and trains on openly available variant data. It’s designed for research-use-only and doesn’t attempt any clinical classification.  I’d love to hear feedback from others working on ML in genomics — particularly about useful features to include, ways to benchmark, or datasets worth adding.  If anyone’s curious about using a version of it internally (e.g., for variant triage in a research setting), you can DM me for details about the commercial license.  Happy to discuss technical stuff openly in the thread — I’m mostly sharing this because it’s been fun applying classical ML to genomics in a practical way	reddit
96	I am an independent researcher. My submissions have recently been published in AI symposiums and in the past I have published in IEEE. I'm looking to upload it to the arxiv I need an endorsement for [CS.AI](http://CS.AI). Thanks in advance.  Endorsement code: 69BL48  [https://arxiv.org/auth/endorse?x=69BL48](https://arxiv.org/auth/endorse?x=69BL48)	reddit
97	Greetings,  We are a small team of 6 people that work on a startup project in our free time (mainly computer vision + some algorithms etc.). So far, we have been using the roboflow platform for labelling, training models etc. However, this is very costly and we cannot justify 60 bucks / month for labelling and limited credits for model training with limited flexibility.     We are looking to see where it is worthwhile to migrate to, without needing too much time to do so and without it being too costly.     Currently, this is our situation:   \- We have a small grant of 500 euros that we can utilize. Aside from that we can also spend from our own money if it's justified. The project produces no revenue yet, we are going to have a demo within this month to see the interest of people and from there see how much time and money we will invest moving forward. In any case we want to have a migration from roboflow set-up to not have delays.  \- We have setup an S3 bucket where we keep our datasets (so far approx. 40GB space) which are constantly growing since we are also doing data collection. We also are renting a VPS where we are hosting CVAT for labelling. These come around 4-7 euros / month. We have set up some basic repositories for drawing data, some basic training workflows which we are trying to figure out, mainly revolving around YOLO, RF-DETR, object detection and segmentation models, some timeseries forecasting, trackers etc. We are playing around with different frameworks so we want to be a bit flexible.  \- We are looking into renting VMs and just using our repos to train models but we also want some easy way to compare runs etc. so we thought something like MLFlow. We tried these a bit but it has an initial learning process and it is time consuming to setup your whole pipeline at first.     \-> What would you guys advice in our case? Is there a specific platform you would recommend us going towards? Do you suggest just running in any VM on the cloud ? If yes, where and what frameworks would you suggest we use for our pipeline? Any suggestions are appreciated and I would be interested to see what computer vision companies use etc. Of course in our case the budget would ideally be less than 500 euros for the next 6 months in costs since we have no revenue and no funding, at least currently.   TL;DR - Which are the most pain-free frameworks/platforms/ways to setup a full pipeline of data gathering -> data labelling -> data storage -> different types of model training/pre-training -> evaluation -> comparison of models -> deployment on our product etc. when we have a 500 euro budget for next 6 months making our lives as much as possible easy while being very flexible and able to  train different models, mess with backbones, transfer learning etc. without issues.     Feel free to ask for any additional information.     Thanks!	reddit
98	There's been some confusion about whether rebuttals should be 2500 characters **per reviewer** or 2500 characters overall. Below I posted a screenshot of the message sent out the last conference (AAAI 2025) which states that it is 2500 characters per reviewer, but this time at AAAI 2026 the wording implies that it is 2500 characters **overall for a single rebuttal covering all reviewers**.  Has anyone been able to get in touch with the AAAI committee for a clarification?     https://preview.redd.it/edmmtrx7nztf1.png?width=1688&format=png&auto=webp&s=f3103ca61e91a43842773f4a193a87f276adfd3d  	reddit
99	Hello everyone,  I would like to collaboratively define a reasonable portfolio to specialize in managing a freelance consulting business as a Data Scientist.  Considering that there are people here who have worked independently as Data Scientists and have observed the types of problems clients usually bring to them.  Please, let us know what kinds of problems or models you have frequently dealt with as freelance consultants. It could be interesting for all of us to share and learn together about the current state of the Data Science market.  I would like to reduce the overwhelming number of Machine Learning models and potential problems in order to build potential specializations for freelance Data Science consultants.  Thank you.	reddit
